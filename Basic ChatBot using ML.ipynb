{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74401ad0-7253-48a2-a162-3a56c9a8764e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff4ee1d-0276-47d1-8249-9812f74fe859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b541b116-4fda-4fe8-956a-3367161b3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_link=urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "get_link=get_link.read()\n",
    "data=bs.BeautifulSoup(get_link,'lxml')\n",
    "data_paragraphs=data.find_all('p')\n",
    "data_text=''\n",
    "for para in data_paragraphs:\n",
    "    data_text+=para.text\n",
    "data_text=data_text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ce4d6b-b5f4-4989-b1c6-4c0702039bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text=re.sub(r'\\[[0-9]*\\]',' ', data_text) \n",
    "data_text=re.sub(r'\\s+',' ', data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4669fa3d-b649-40fd-8749-2e64c7f16b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6d289a-e128-4c66-a369-a07ae8679e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd212877-0c01-43a4-acd2-063fabbfcc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence='its too cold outside'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7dbbbfe-0636-439d-8022-3374990c6f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['its', 'too', 'cold', 'outside']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cecb13ab-ca39-4c5c-b952-0f0930d8c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentences=nltk.sent_tokenize(data_text)\n",
    "data_words= nltk.word_tokenize(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7513da5b-eb74-49de-b81e-70c4ffd7dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnlemmatizer=nltk.stem.WordNetLemmatizer() \n",
    "def perform_lemmatization(tokens): \n",
    " return [wnlemmatizer.lemmatize(token) for token in tokens] \n",
    "punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
    "\n",
    "def get_processed_text(document): \n",
    " return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31319e24-7509-4fa5-84c8-88df73d5f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_input =(\"hey\", \"good morning\", \"good evening\", \"morning\", \"evening\", \"hi\", \"whatsup\") \n",
    "greeting_response= [\"hey\", \"hey hows you?\", \"nods\", \"hello, how you doing\", \"hello\", \"Welcome, I am good and you\"]\n",
    "\n",
    "def generate_greeting_response(greeting):\n",
    "    for token in greeting.split(): \n",
    "      if token.lower() in greeting_input: \n",
    "        return random.choice(greeting_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62d49d82-02ac-4666-a579-b293cb814d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5acc5ae-2dc0-40c9-8117-808269a33ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input): \n",
    " bot_response = ''\n",
    " data_sentences.append(user_input) \n",
    " word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english') \n",
    " all_word_vectors= word_vectorizer.fit_transform(data_sentences) \n",
    " similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors) \n",
    " similar_sentence_number = similar_vector_values.argsort()[0][-2] \n",
    " matched_vector=similar_vector_values.flatten() \n",
    " matched_vector.sort() \n",
    " vector_matched =matched_vector[-2] \n",
    " if vector_matched == 0: \n",
    "   bot_response= bot_response + \"I am sorry, I could not understand you\" \n",
    "   return bot_response \n",
    " else:\n",
    "   bot_response= bot_response+data_sentences[similar_sentence_number]\n",
    "   return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d6beac7-51e7-4a4b-a351-41c02ab4c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\hp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e01be5-27b9-4fb8-9b74-4adef9bd4756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I am from AI Sciences, you can ask me any question regarding AI\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is artificial intelligence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al Sciences:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " artificial intelligence (ai), in its broadest sense, is intelligence exhibited by machines, particularly computer systems.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what are ai goals?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al Sciences:these ai programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is natural language processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al Sciences:natural language processing (nlp) allows programs to read, write and communicate in human languages such as english.\n"
     ]
    }
   ],
   "source": [
    "continue_dialogue = True \n",
    "print(\"Hello I am from AI Sciences, you can ask me any question regarding AI\") \n",
    "while(continue_dialogue==True): \n",
    "  human_text= input() \n",
    "  human_text=human_text.lower() \n",
    "  if human_text!='bye': \n",
    "     if human_text=='thanks' or human_text == 'thank you very much' or human_text == 'thank you': \n",
    "         continue_dialogue = False \n",
    "         print(\"AI Sciences: Most Welcome\") \n",
    "     else: \n",
    "        if generate_greeting_response(human_text) !=None: \n",
    "           print(\"Al Sciences:\"+ generate_greeting_response(human_text))  \n",
    "        else: \n",
    "           print(\"Al Sciences:\", end=\"\") \n",
    "           print(generate_response(human_text)) \n",
    "           data_sentences.remove(human_text) \n",
    "  else: \n",
    "    continue_dialogue = False \n",
    "    print(\"Al Sciences: Good bye and take care of yourself.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52caa9c-f27b-4000-b127-3535c98d179f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
